
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Surrogates &#8212; aaaaa v1.0 documentation</title>
    <link rel="stylesheet" href="_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="_static/alabaster.css" type="text/css" />
    <script id="documentation_options" data-url_root="./" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
   
  <link rel="stylesheet" href="_static/custom.css" type="text/css" />
  
  
  <meta name="viewport" content="width=device-width, initial-scale=0.9, maximum-scale=0.9" />

  </head><body>
  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          

          <div class="body" role="main">
            
  <div class="section" id="module-UQpy.Surrogates">
<span id="surrogates"></span><span id="surrogates-doc"></span><h1>Surrogates<a class="headerlink" href="#module-UQpy.Surrogates" title="Permalink to this headline">¶</a></h1>
<p>This module contains functionality for all the surrogate methods supported in UQpy.</p>
<div class="section" id="stochatic-reduced-order-models-sroms">
<h2>Stochatic Reduced Order Models - SROMs<a class="headerlink" href="#stochatic-reduced-order-models-sroms" title="Permalink to this headline">¶</a></h2>
<p>An SROM is a sample-based surrogate for probability models. An SROM takes a set of samples and attributes of a distribution and optimizes the sample probability weights according to the method in <a class="footnote-reference brackets" href="#id3" id="id1">1</a>. More specifically, an SROM constructs a reduce order model for arbitrary random variables <cite>X</cite> as follows.</p>
<div class="math notranslate nohighlight">
\[\begin{split}\tilde{X} =  \begin{cases} x_1 &amp; probability \text{  }p_1^{(opt)} \\ &amp; \vdots \\ x_m &amp; probability \text{  }p_m^{(opt)} \end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\tilde{X}\)</span> is defined by an arbitrary set of samples <span class="math notranslate nohighlight">\(x_1, \dots, x_m\)</span> defined over the same support as <span class="math notranslate nohighlight">\(X\)</span> (but not necessarily drawn from its probability distribution) and their assigned probability weights. The probability weights are defined such that the total error between the sample empirical probability distribution, moments and correlation of <span class="math notranslate nohighlight">\(\tilde{X}\)</span> and those of the random variable <cite>X</cite> is minimized. This optimization problem can be express as:</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp; \min_{\mathbf{p}}  \sum_{u=1}^3 \alpha_u e_u(\mathbf{p}) \\ &amp; \text{s.t.} \sum_{k=1}^m p_k =1 \quad and \quad p_k \geq 0, \quad k=1,2,\dots,m\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_1\)</span>, <span class="math notranslate nohighlight">\(\alpha_2\)</span>, <span class="math notranslate nohighlight">\(\alpha_3 \geq 0\)</span> are constants defining the relative importance of the marginal distribution, moments and correlation error between the reduce order model and actual random variables in the objective function.</p>
<div class="math notranslate nohighlight">
\[\begin{split}&amp;  e_{1}(p)=\sum\limits_{i=1}^d \sum\limits_{k=1}^m w_{F}(x_{k,i};i)(\hat{F}_{i}(x_{k,i})-F_{i}(x_{k,i}))^2  \\ &amp; e_{2}(p)=\sum\limits_{i=1}^d \sum\limits_{r=1}^2 w_{\mu}(r;i)(\hat{\mu}(r;i)-\mu(r;i))^2 \\ &amp; e_{3}(p)=\sum\limits_{i,j=1,...,d ; j&gt;i}  w_{R}(i,j)(\hat{R}(i,j)-R(i,j))^2\end{split}\]</div>
<p>Here, <span class="math notranslate nohighlight">\(F(x_{k,i})\)</span> and <span class="math notranslate nohighlight">\(\hat{F}(x_{k,i})\)</span> denote the marginal cumulative distributions of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\tilde{X}}\)</span> (reduced order model) evaluated at point <span class="math notranslate nohighlight">\(x_{k,i}\)</span>, <span class="math notranslate nohighlight">\(\mu(r; i)\)</span> and <span class="math notranslate nohighlight">\(\hat{\mu}(r; i)\)</span> are the marginal moments of order <cite>r</cite> for variable <cite>i</cite>, and <span class="math notranslate nohighlight">\(R(i,j)\)</span> and <span class="math notranslate nohighlight">\(\hat{R}(i,j)\)</span> are correlation matrices of <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> and <span class="math notranslate nohighlight">\(\mathbf{\tilde{X}}\)</span> evaluted for components <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(x_j\)</span>. Note also that <cite>m</cite> is the number of sample points and <cite>d</cite> is the number of random variables.</p>
<div class="section" id="srom-class-descriptions">
<h3>SROM Class Descriptions<a class="headerlink" href="#srom-class-descriptions" title="Permalink to this headline">¶</a></h3>
<dl class="py class">
<dt id="UQpy.Surrogates.SROM">
<em class="property">class </em><code class="sig-prename descclassname">UQpy.Surrogates.</code><code class="sig-name descname">SROM</code><span class="sig-paren">(</span><em class="sig-param"><span class="n">samples</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cdf_target</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">moments</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weights_errors</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weights_distribution</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weights_moments</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">weights_correlation</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">properties</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">cdf_target_params</span><span class="o">=</span><span class="default_value">None</span></em>, <em class="sig-param"><span class="n">correlation</span><span class="o">=</span><span class="default_value">None</span></em><span class="sig-paren">)</span><a class="reference internal" href="_modules/UQpy/Surrogates.html#SROM"><span class="viewcode-link">[source]</span></a><a class="headerlink" href="#UQpy.Surrogates.SROM" title="Permalink to this definition">¶</a></dt>
<dd><p>Description:</p>
<blockquote>
<div><p>Stochastic Reduced Order Model(SROM) provide a low-dimensional, discrete approximation of a given random
quantity.
SROM generates a discrete approximation of continuous random variables. The probabilities/weights are
considered to be the parameters for the SROM and they can be obtained by minimizing the error between the
marginal distributions, first and second order moments about origin and correlation between random variables
References:
M. Grigoriu, “Reduced order models for random functions. Application to stochastic problems”,</p>
<blockquote>
<div><p>Applied Mathematical Modelling, Volume 33, Issue 1, Pages 161-175, 2009.</p>
</div></blockquote>
</div></blockquote>
<dl>
<dt>Input:</dt><dd><dl class="field-list">
<dt class="field-odd">param samples</dt>
<dd class="field-odd"><p>An array/list of samples corresponding to each random variables</p>
</dd>
<dt class="field-even">param cdf_target</dt>
<dd class="field-even"><p>A list of Cumulative distribution functions of random variables</p>
</dd>
<dt class="field-odd">type cdf_target</dt>
<dd class="field-odd"><p>list str or list function</p>
</dd>
<dt class="field-even">param cdf_target_params</dt>
<dd class="field-even"><p>Parameters of distribution</p>
</dd>
<dt class="field-odd">type cdf_target_params</dt>
<dd class="field-odd"><p>list</p>
</dd>
<dt class="field-even">param moments</dt>
<dd class="field-even"><p>A list containing first and second order moment about origin of all random variables</p>
</dd>
<dt class="field-odd">param weights_errors</dt>
<dd class="field-odd"><p>Weights associated with error in distribution, moments and correlation.
Default: weights_errors = [1, 0.2, 0]</p>
</dd>
<dt class="field-even">type weights_errors</dt>
<dd class="field-even"><p>list</p>
</dd>
<dt class="field-odd">param properties</dt>
<dd class="field-odd"><p>A list of booleans representing properties, which are required to match in reduce
order model. This class focus on reducing errors in distribution, first order moment
about origin, second order moment about origin and correlation of samples.
Default: properties = [True, True, True, False]
Example: properties = [True, True, False, False] will minimize errors in distribution and
errors in first order moment about origin in reduce order model.</p>
</dd>
<dt class="field-even">type properties</dt>
<dd class="field-even"><p>list</p>
</dd>
<dt class="field-odd">param weights_distribution</dt>
<dd class="field-odd"><p>An list or array containing weights associated with different samples.
Options:</p>
<blockquote>
<div><p>If weights_distribution is None, then default value is assigned.
If size of weights_distribution is 1xd, then it is assigned as dot product</p>
<blockquote>
<div><p>of weights_distribution and default value.</p>
</div></blockquote>
<p>Otherwise size of weights_distribution should be equal to Nxd.</p>
</div></blockquote>
<p>Default: weights_distribution = Nxd dimensional array with all elements equal
to 1.</p>
</dd>
<dt class="field-even">param weights_moments</dt>
<dd class="field-even"><p>An array of dimension 2xd, where ‘d’ is number of random variables. It contain
weights associated with moments.
Options:</p>
<blockquote>
<div><p>If weights_moments is None, then default value is assigned.
If size of weights_moments is 1xd, then it is assigned as dot product</p>
<blockquote>
<div><p>of weights_moments and default value.</p>
</div></blockquote>
<p>Otherwise size of weights_distribution should be equal to 2xd.</p>
</div></blockquote>
<p>Default: weights_moments = Square of reciprocal of elements of moments.</p>
</dd>
<dt class="field-odd">type weights_moments</dt>
<dd class="field-odd"><p>ndarray or list (float)</p>
</dd>
<dt class="field-even">param weights_correlation</dt>
<dd class="field-even"><p>An array of dimension dxd, where ‘d’ is number of random variables. It contain
weights associated with correlation of random variables.
Default: weights_correlation = dxd dimensional array with all elements equal to
1.</p>
</dd>
<dt class="field-odd">param correlation</dt>
<dd class="field-odd"><p>Correlation matrix between random variables.</p>
</dd>
</dl>
</dd>
<dt>Output:</dt><dd><dl class="field-list simple">
<dt class="field-odd">return</dt>
<dd class="field-odd"><p>SROM.samples: Last column contains the probabilities/weights defining discrete approximation of
continuous random variables.</p>
</dd>
<dt class="field-even">rtype</dt>
<dd class="field-even"><p>SROM.samples: ndarray</p>
</dd>
</dl>
</dd>
</dl>
</dd></dl>

</div>
</div>
<div class="section" id="gaussian-process-regression-kriging">
<h2>Gaussian Process Regression / Kriging<a class="headerlink" href="#gaussian-process-regression-kriging" title="Permalink to this headline">¶</a></h2>
<p>The <code class="docutils literal notranslate"><span class="pre">Kriging</span></code> class defines an approximate surrogate model or response surface which can be used to predict the model response and its uncertainty at points where the model has not been previously evaluated. Kriging gives the best unbiased linear predictor at the interpolated points. This class generates a model <span class="math notranslate nohighlight">\(\hat{y}\)</span> that express the response as a realization of regression model and Gaussian random process as:</p>
<div class="math notranslate nohighlight">
\[\hat{y}(x) = \mathcal{F}(\beta, x) + z(x).\]</div>
<p>The regression model (<span class="math notranslate nohighlight">\(\mathcal{F}\)</span>) is given as a linear combination of ‘<span class="math notranslate nohighlight">\(p\)</span>’ chosen scalar basis functions as:</p>
<div class="math notranslate nohighlight">
\[\mathcal{F}(\beta, x) = \beta_1 f_1(x) + \dots + \beta_p f_p(x) = f(x)^T \beta.\]</div>
<p>The random process <span class="math notranslate nohighlight">\(z(x)\)</span> has zero mean and its covariance is defined through the separable correlation function:</p>
<div class="math notranslate nohighlight">
\[E\big[z(s)z(x)] = \sigma^2 \mathcal{R}(\theta, s, x)\]</div>
<p>where,</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}(s, x; \theta) = \prod_{i=1}^d \mathcal{R}_i(s_i, x_i; \theta_i),\]</div>
<p>and <span class="math notranslate nohighlight">\(\theta\)</span> are a set of hyperparameters generally governing the correlation length of the model determined by maximixing the log-likelihood function</p>
<div class="math notranslate nohighlight">
\[\text{log}(p(y|x, \theta)) = -\frac{1}{2}y^T \mathcal{R}^{-1} y - \frac{1}{2}\text{log}(|\mathcal{R}|) - \frac{n}{2}\text{log}(2\pi)\]</div>
<p>The correlation is evaluated between a set of existing sample points <span class="math notranslate nohighlight">\(s\)</span> and points <span class="math notranslate nohighlight">\(x\)</span> in the domain of interest to form the correlation matrix <span class="math notranslate nohighlight">\(R\)</span>, and the basis functions are evaluated at the sample points <span class="math notranslate nohighlight">\(s\)</span> to form the matrix <span class="math notranslate nohighlight">\(F\)</span>. Using these matrices, the regression coefficients, <span class="math notranslate nohighlight">\(\beta\)</span>, and process variance, <span class="math notranslate nohighlight">\(\sigma^2\)</span> are computed as</p>
<div class="math notranslate nohighlight">
\[\begin{split}(F^T R^{-1} F)\beta^* &amp; = F^T R^{-1} Y \\ \sigma^2 &amp; = \frac{1}{m} (Y - F\beta^*)^T R{-1}(Y - F\beta^*)\end{split}\]</div>
<p>The final predictor function is then given by:</p>
<div class="math notranslate nohighlight">
\[\hat{y}(x) = f(x)^T \beta^* + r(x)^T R^{-1}(Y - F\beta^*)\]</div>
<div class="section" id="regression-models">
<h3>Regression Models<a class="headerlink" href="#regression-models" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Kriging</span></code> class offers a variety of built-in regression models, specified by the <cite>reg_model</cite> input described below.</p>
<div class="section" id="ordinary-kriging">
<h4>Ordinary Kriging<a class="headerlink" href="#ordinary-kriging" title="Permalink to this headline">¶</a></h4>
<p>In ordinary Kriging, the regression model is assumed to take a constant value such that</p>
<div class="math notranslate nohighlight">
\[\mathcal{F}(\beta, x) = \beta_0\]</div>
</div>
<div class="section" id="universal-kriging">
<h4>Universal Kriging<a class="headerlink" href="#universal-kriging" title="Permalink to this headline">¶</a></h4>
<p>In universal Kriging, the regression model is assumed to take a general functional form. The <code class="docutils literal notranslate"><span class="pre">Kriging</span></code> class currenly supports two univeral Kriging models, the linear regression model given by:</p>
<div class="math notranslate nohighlight">
\[\mathcal{F}(\beta, x) = \beta_0 = \sum_{i=1}^d \beta_i x_i\]</div>
<p>and the quadratic regression model given by:</p>
<div class="math notranslate nohighlight">
\[\mathcal{F}(\beta, x) = \beta_0 = \sum_{i=1}^d \beta_i x_i + \sum_{i=1}^d \sum_{j=1}^d \beta_{ij} x_i x_j\]</div>
</div>
<div class="section" id="user-defined-regression-model">
<h4>User-Defined Regression Model<a class="headerlink" href="#user-defined-regression-model" title="Permalink to this headline">¶</a></h4>
<p>Adding a new regression model to the <code class="docutils literal notranslate"><span class="pre">Kriging</span></code> class is straightforward. This is done by creating a new method that evaluates the basis functions and the Jacobian. This method may be passed directly as a callable to the <cite>reg_model</cite> input of the <code class="docutils literal notranslate"><span class="pre">Kriging</span></code> class. This new method should takes as input the samples points at which to evaluate the model and return two arrays containing the value of the basis functions and the Jacobian at these sample points.</p>
<p>The first output of this function should be a two dimensional numpy array with the first dimension being the number of samples and the second dimension being the number of basis functions.</p>
<p>The second output (i.e. Jacobian of basis function) is a three dimensional numpy array with the first dimension being the number of samples, the second dimension being the number of variables and the third dimension being the number of basis functions.</p>
<p>An example user-defined model is given below:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">constant</span><span class="p">(</span><span class="n">points</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">fx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">([</span><span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">jf</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">([</span><span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">points</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="mi">1</span><span class="p">])</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">fx</span><span class="p">,</span> <span class="n">jf</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="correlation-models">
<h3>Correlation Models<a class="headerlink" href="#correlation-models" title="Permalink to this headline">¶</a></h3>
<p>The <code class="docutils literal notranslate"><span class="pre">Kriging</span></code> class offers a variety of built-in correlation models, specified by the <cite>corr_model</cite> input described below.</p>
<div class="section" id="exponential-correlation">
<h4>Exponential Correlation<a class="headerlink" href="#exponential-correlation" title="Permalink to this headline">¶</a></h4>
<p>The exponential correlation model takes the following form:</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}_i(h_i, \theta_i) = \exp\bigg[ -\dfrac{|h_i|}{\theta_i}\bigg]\]</div>
<p>where <span class="math notranslate nohighlight">\(h_i = s_i-x_i\)</span>.</p>
</div>
<div class="section" id="gaussian-correlation">
<h4>Gaussian Correlation<a class="headerlink" href="#gaussian-correlation" title="Permalink to this headline">¶</a></h4>
<p>The Gaussian correlation model takes the following form:</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}_i(h_i, \theta_i) = \exp\bigg[ -\bigg(\dfrac{h_i}{\theta_i}\bigg)^2\bigg]\]</div>
<p>where <span class="math notranslate nohighlight">\(h_i = s_i-x_i\)</span>.</p>
</div>
<div class="section" id="linear-correlation">
<h4>Linear Correlation<a class="headerlink" href="#linear-correlation" title="Permalink to this headline">¶</a></h4>
<p>The linear correlation model takes the following form:</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}_i(h_i, \theta_i) = \max \bigg(0, 1-\dfrac{|h_i|}{\theta_i}\bigg)\]</div>
<p>where <span class="math notranslate nohighlight">\(h_i = s_i-x_i\)</span>.</p>
</div>
<div class="section" id="spherical-correlation">
<h4>Spherical Correlation<a class="headerlink" href="#spherical-correlation" title="Permalink to this headline">¶</a></h4>
<p>The spherical correlation model takes the following form:</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}_i(h_i, \theta_i) = 1 - 1.5\xi_i + 0.5\xi_i^3\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi_i =  \min \bigg(1, \dfrac{|h_i|}{\theta_i}\bigg)\)</span> and <span class="math notranslate nohighlight">\(h_i = s_i-x_i\)</span>.</p>
</div>
<div class="section" id="cubic-correlation">
<h4>Cubic Correlation<a class="headerlink" href="#cubic-correlation" title="Permalink to this headline">¶</a></h4>
<p>The cubic correlation model takes the following form:</p>
<div class="math notranslate nohighlight">
\[\mathcal{R}_i(h_i, \theta_i) = 1 - 3\xi_i^2 + 2\xi_i^3\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi_i =  \min \bigg(1, \dfrac{|h_i|}{\theta_i}\bigg)\)</span> and <span class="math notranslate nohighlight">\(h_i = s_i-x_i\)</span>.</p>
</div>
<div class="section" id="spline-correlation">
<h4>Spline Correlation<a class="headerlink" href="#spline-correlation" title="Permalink to this headline">¶</a></h4>
<p>The spline correlation model takes the following form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\mathcal{R}_i(h_i, \theta_i) = \begin{cases} 1-1.5\xi_i^2+30\xi_i^3, &amp; 0\leq \xi_i \leq 0.02 \\  1.25(1-\xi_i)^3, &amp; 0.2&lt;\xi_i&lt;1 \\ 0, &amp; \xi_i \geq 1\end{cases}\end{split}\]</div>
<p>where <span class="math notranslate nohighlight">\(\xi_i =  \min \bigg(1, \dfrac{|h_i|}{\theta_i}\bigg)\)</span> and <span class="math notranslate nohighlight">\(h_i = s_i-x_i\)</span>.</p>
</div>
<div class="section" id="user-defined-correlation">
<h4>User-Defined Correlation<a class="headerlink" href="#user-defined-correlation" title="Permalink to this headline">¶</a></h4>
<p>Adding a new correlation model to the <code class="docutils literal notranslate"><span class="pre">Kriging</span></code> class is straightforward. This is done by creating a new method that evaluates the correlation matrix, it’s derivative with respect to the variables and it’s derivative with respect to the hyperparameters. This method takes as input the new points, training points, hyperparameters and two indicators for the computation of the derivative of correlation matrix (i.e. <cite>dt</cite> and <cite>dx</cite>).</p>
<p>If both indicators are false, then the method should return correlation matrix, i.e. a 2-D array with first dimension being the number of points and second dimension being the number of training points.</p>
<p>If <cite>dx</cite> parameter is True, the method should return the derivative of the correlation matrix respect to the variables, i.e. a 3-D array with first dimension being the number of points, second dimension being the number of training points and third dimension being the number of variables.</p>
<p>If <cite>dt</cite> is True, then the method should return the correlation matrix and it’s derivative with respect to the hyperparameters, i.e. a 3-D array with first dimension being the number of points, second dimension being the number of training points and third dimension being the number of variables.</p>
<p>An example user-defined model is given below:</p>
<div class="doctest highlight-default notranslate"><div class="highlight"><pre><span></span><span class="gp">&gt;&gt;&gt; </span><span class="k">def</span> <span class="nf">Gaussian</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">s</span><span class="p">,</span> <span class="n">params</span><span class="p">,</span> <span class="n">dt</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">dx</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">x</span><span class="p">,</span> <span class="n">s</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="n">np</span><span class="o">.</span><span class="n">atleast_2d</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="c1"># Create stack matrix, where each block is x_i with all s</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">stack</span> <span class="o">=</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">swapaxes</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">atleast_3d</span><span class="p">(</span><span class="n">x</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">s</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="n">rx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="n">params</span> <span class="o">*</span> <span class="p">(</span><span class="n">stack</span> <span class="o">**</span> <span class="mi">2</span><span class="p">),</span> <span class="n">axis</span><span class="o">=</span><span class="mi">2</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">dt</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">drdt</span> <span class="o">=</span> <span class="o">-</span><span class="p">(</span><span class="n">stack</span> <span class="o">**</span> <span class="mi">2</span><span class="p">)</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">rx</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">rx</span><span class="p">,</span> <span class="n">drdt</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">if</span> <span class="n">dx</span><span class="p">:</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="n">drdx</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">params</span> <span class="o">*</span> <span class="n">stack</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">tile</span><span class="p">(</span><span class="n">rx</span><span class="p">,</span> <span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)),</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>
<span class="gp">&gt;&gt;&gt; </span>        <span class="k">return</span> <span class="n">rx</span><span class="p">,</span> <span class="n">drdx</span>
<span class="gp">&gt;&gt;&gt; </span>    <span class="k">return</span> <span class="n">rx</span>
</pre></div>
</div>
</div>
</div>
<div class="section" id="kriging-class-descriptions">
<h3>Kriging Class Descriptions<a class="headerlink" href="#kriging-class-descriptions" title="Permalink to this headline">¶</a></h3>
</div>
</div>
<div class="section" id="polynomial-chaos-expansion-pce">
<h2>Polynomial Chaos Expansion - PCE<a class="headerlink" href="#polynomial-chaos-expansion-pce" title="Permalink to this headline">¶</a></h2>
<p>Polynomial Chaos Expansions (PCE) represent a class of methods which employ orthonormal polynomials to construct approximate response surfaces (metamodels or surrogate models) to identify a mapping between inputs and outputs of a numerical model <a class="footnote-reference brackets" href="#id4" id="id2">2</a>. PCE methods can be directly used for moment estimation and sensitivity analysis (Sobol indices). A PCE object can be instantiated from the class <code class="docutils literal notranslate"><span class="pre">PCE</span></code>. The method can be used for models of both one-dimensional and multi-dimensional outputs.</p>
<p>Let us consider a computational model <span class="math notranslate nohighlight">\(Y = \mathcal{M}(x)\)</span>, with <span class="math notranslate nohighlight">\(Y \in \mathbb{R}\)</span> and a random vector with independent components <span class="math notranslate nohighlight">\(X \in \mathbb{R}^M\)</span> described by the joint probability density function <span class="math notranslate nohighlight">\(f_X\)</span>. The polynomial chaos expansion of <span class="math notranslate nohighlight">\(\mathcal{M}(x)\)</span> is</p>
<div class="math notranslate nohighlight">
\[Y = \mathcal{M}(x) = \sum_{\alpha \in \mathbb{N}^M} y_{\alpha} \Psi_{\alpha} (X)\]</div>
<p>where the <span class="math notranslate nohighlight">\(\Psi_{\alpha}(X)\)</span> are multivariate polynomials orthonormal with respect to <span class="math notranslate nohighlight">\(f_X\)</span> and <span class="math notranslate nohighlight">\(y_{\alpha} \in \mathbb{R}\)</span> are the corresponding coefficients.</p>
<p>Practically, the above sum needs to be truncated to a finite sum so that <span class="math notranslate nohighlight">\(\alpha \in A\)</span> where <span class="math notranslate nohighlight">\(A \subset \mathbb{N}^M\)</span>. The polynomial basis <span class="math notranslate nohighlight">\(\Psi_{\alpha}(X)\)</span> is built from a set of <em>univariate orthonormal polynomials</em> <span class="math notranslate nohighlight">\(\phi_j^{i}(x_i)\)</span> which satisfy the following relation</p>
<div class="math notranslate nohighlight">
\[\Big&lt; \phi_j^{i}(x_i),\phi_k^{i}(x_i) \Big&gt; = \int_{D_{X_i}} \phi_j^{i}(x_i),\phi_k^{i}(x_i) f_{X_i}(x_i)dx_i = \delta_{jk}\]</div>
<p>The multivariate polynomials <span class="math notranslate nohighlight">\(\Psi_{\alpha}(X)\)</span> are assembled as the tensor product of their univariate counterparts as follows</p>
<div class="math notranslate nohighlight">
\[\Psi_{\alpha}(X) = \prod_{i=1}^M \phi_{\alpha_i}^{i}(x_i)\]</div>
<p>which are also orthonormal.</p>
<div class="section" id="pce-class-descriptions">
<h3>PCE Class Descriptions<a class="headerlink" href="#pce-class-descriptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="univariate-orthonormal-polynomials">
<h4>Univariate Orthonormal Polynomials<a class="headerlink" href="#univariate-orthonormal-polynomials" title="Permalink to this headline">¶</a></h4>
<p>Different families of univariate polynomials can be used for the PCE method. These polynomials must always be orthonormal with respect to the arbitrary distribution. In UQpy, two families of polynomials are currently available that can be used from their corresponding classes, namely the <code class="docutils literal notranslate"><span class="pre">Legendre</span></code> and <code class="docutils literal notranslate"><span class="pre">Hermite</span></code> polynomial class, appropriate for data generated from a Uniform and a Normal distribution respectively.</p>
</div>
</div>
<div class="section" id="polynomials-class-descriptions">
<h3>Polynomials Class Descriptions<a class="headerlink" href="#polynomials-class-descriptions" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="legendre-class-descriptions">
<h3>Legendre Class Descriptions<a class="headerlink" href="#legendre-class-descriptions" title="Permalink to this headline">¶</a></h3>
</div>
<div class="section" id="hermite-class-descriptions">
<h3>Hermite Class Descriptions<a class="headerlink" href="#hermite-class-descriptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="calculation-of-the-pce-coefficients">
<h4>Calculation of the PCE coefficients<a class="headerlink" href="#calculation-of-the-pce-coefficients" title="Permalink to this headline">¶</a></h4>
<p>Several methods exist for the calculation of the PCE coefficients. In UQpy, three non-intrusive methods can be used, namely the Least Squares regression (<code class="docutils literal notranslate"><span class="pre">PolyChaosLstsq</span></code> class), the LASSO regression (<code class="docutils literal notranslate"><span class="pre">PolyChaosLasso</span></code> class) and Ridge regression (<code class="docutils literal notranslate"><span class="pre">PolyChaosRidge</span></code> class) methods.</p>
</div>
<div class="section" id="least-squares-regression">
<h4>Least Squares Regression<a class="headerlink" href="#least-squares-regression" title="Permalink to this headline">¶</a></h4>
<p>Least Squares regression is a method for estimating the parameters of a linear regression model. The goal is to minimize the sum of squares of the differences of the observed dependent variable and the predictions of the regression model. In other words, we seek for the vector <span class="math notranslate nohighlight">\(\beta\)</span>, that approximatively solves the equation <span class="math notranslate nohighlight">\(X \beta \approx y\)</span>. If matrix <span class="math notranslate nohighlight">\(X\)</span> is square then the solution is exact.</p>
<p>If we assume that the system cannot be solved exactly, since the number of equations <span class="math notranslate nohighlight">\(n\)</span> is not equal to the number of unknowns <span class="math notranslate nohighlight">\(p\)</span>, we are seeking the solution that is associated with the smallest difference between the right-hand-side and left-hand-side of the equation. Therefore, we are looking for the solution that satisfies the following</p>
<div class="math notranslate nohighlight">
\[\hat{\beta} = \underset{\beta}{\arg\min} \| y - X \beta \|_{2}\]</div>
<p>where <span class="math notranslate nohighlight">\(\| \cdot \|_{2}\)</span> is the standard <span class="math notranslate nohighlight">\(L^{2}\)</span> norm in the <span class="math notranslate nohighlight">\(n\)</span>-dimensional Eucledian space <span class="math notranslate nohighlight">\(\mathbb{R}^{n}\)</span>. The above function is also known as the cost function of the linear regression.</p>
<p>The equation may be under-, well-, or over-determined. In the context of Polynomial Chaos Expansion (PCE) the computed vector corresponds to the polynomial coefficients. The above method can be used from the class <code class="docutils literal notranslate"><span class="pre">PolyChaosLstsq</span></code>.</p>
</div>
</div>
<div class="section" id="polychaoslstsq-class-descriptions">
<h3>PolyChaosLstsq Class Descriptions<a class="headerlink" href="#polychaoslstsq-class-descriptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="lasso-regression">
<h4>Lasso Regression<a class="headerlink" href="#lasso-regression" title="Permalink to this headline">¶</a></h4>
<p>A drawback of using Least Squares regression for calculating the PCE coefficients, is that this method considers all the features (polynomials) to be equally relevant for the prediction. This technique often results to overfitting and complex models that do not have the ability to generalize well on unseen data. For this reason, the Least Absolute Shrinkage and Selection Operator or LASSO can be employed (from the <code class="docutils literal notranslate"><span class="pre">PolyChaosLasso</span></code> class). This method, introduces an <span class="math notranslate nohighlight">\(L_{1}\)</span> penalty term (which encourages sparcity) in the loss function of linear regression as follows</p>
<div class="math notranslate nohighlight">
\[\hat{\beta} = \underset{\beta}{\arg\min} \{ \frac{1}{N} \| y - X \beta \|_{2} + \lambda \| \beta \|_{1} \}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is called the regularization strength.</p>
<p>Parameter <span class="math notranslate nohighlight">\(\lambda\)</span> controls the level of penalization. When it is close to zero, Lasso regression is identical to Least Squares regression, while in the extreme case when it is set to be infinite all coefficients are equal to zero.</p>
<p>The Lasso regression model needs to be trained on the data, and for this gradient descent is used for the optimization of coefficients. In gradient descent, the gradient of the loss function with respect to the weights/coefficients <span class="math notranslate nohighlight">\(\nabla Loss_{\beta}\)</span> is used and deducted from <span class="math notranslate nohighlight">\(\beta^{i}\)</span> at each iteration as follows</p>
<div class="math notranslate nohighlight">
\[\beta^{i+1} = \beta^{i} - \epsilon \nabla Loss_{\beta}^{i}\]</div>
<p>where <span class="math notranslate nohighlight">\(i\)</span> is the iteration step, and <span class="math notranslate nohighlight">\(\epsilon\)</span> is the learning rate (gradient descent step) with a value larger than zero.</p>
</div>
</div>
<div class="section" id="polychaoslasso-class-descriptions">
<h3>PolyChaosLasso Class Descriptions<a class="headerlink" href="#polychaoslasso-class-descriptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="ridge-regression">
<h4>Ridge Regression<a class="headerlink" href="#ridge-regression" title="Permalink to this headline">¶</a></h4>
<p>Ridge regression (also known as <span class="math notranslate nohighlight">\(L_{2}\)</span> regularization) is another variation of the linear regression method and a special case of the Tikhonov regularization. Similarly to the Lasso regression, it introduces an additional penalty term, however Ridge regression uses an <span class="math notranslate nohighlight">\(L_{2}\)</span> norm in the loss function as follows</p>
<div class="math notranslate nohighlight">
\[\hat{\beta} = \underset{\beta}{\arg\min} \{ \frac{1}{N} \| y - X \beta \|_{2} + \lambda \| \beta \|_{2} \}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is called the regularization strength.</p>
<p>Due to the penalization of terms, Ridge regression constructs models that are less prone to overfitting. The level of penalization is similarly controlled by the hyperparameter <span class="math notranslate nohighlight">\(\lambda\)</span> and the coefficients are optimized with gradient descent. The Ridge regression method can be used from the <code class="docutils literal notranslate"><span class="pre">PolyChaosRidge</span></code> class.</p>
</div>
</div>
<div class="section" id="polychaosridge-class-descriptions">
<h3>PolyChaosRidge Class Descriptions<a class="headerlink" href="#polychaosridge-class-descriptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="error-estimation">
<h4>Error Estimation<a class="headerlink" href="#error-estimation" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">ErrorEstimation</span></code> class can be used to estimate the accuracy of the PCE predictor. Here, we compute the generalization error  in the form of the relative mean squared error normalized by the model variance. The user must create an independent validation dataset <span class="math notranslate nohighlight">\([x_{val}, y_{val} = M(x_{val})]\)</span> (i.e. a set of inputs and outputs of the computational model). The validation error is computed as</p>
<div class="math notranslate nohighlight">
\[\epsilon_{val} = \frac{N-1}{N} \Bigg[\frac{\sum_{i=1}^{N} (M(x_{val}^{(i)}) - M^{PCE}(x_{val}^{(i)}) )^{2} }{\sum_{i=1}^{N} (M(x_{val}^{(i)}) - \hat{\mu}_{Y_{val}})^{2}} \Bigg]\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\mu}_{Y_{val}}\)</span> is the sample mean value of the validation dataset output.</p>
<p>In case where the computational model is very expensive, the use of an alternative error measure is recommended, for example the cross-validation error which partitions the existing training dataset into subsets and computes the error as the average of the individual errors of each subset.</p>
</div>
</div>
<div class="section" id="errorestimation-class-descriptions">
<h3>ErrorEstimation Class Descriptions<a class="headerlink" href="#errorestimation-class-descriptions" title="Permalink to this headline">¶</a></h3>
<div class="section" id="moment-estimation">
<h4>Moment Estimation<a class="headerlink" href="#moment-estimation" title="Permalink to this headline">¶</a></h4>
<p>The <code class="docutils literal notranslate"><span class="pre">MomentEstimation</span></code> class can be used for the calculation of the first two moments of the PCE model directly from the PCE coefficients. This is possible due to the orthonormality of the polynomial basis.</p>
<p>The first moment (mean value) is calculated as</p>
<div class="math notranslate nohighlight">
\[\mu_{PCE} = \mathbb{E} [ \mathcal{M}^{PCE}(x)] = y_{0}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_{0}\)</span> is the first PCE coefficient associated with the constant term.</p>
<p>The second moment (variance) is calculated as</p>
<div class="math notranslate nohighlight">
\[\sigma^{2}_{PCE} = \mathbb{E} [( \mathcal{M}^{PCE}(x) - \mu_{PCE} )^{2} ] = \sum_{i=1}^{p} y_{i}\]</div>
<p>where <span class="math notranslate nohighlight">\(p\)</span> is the number of polynomials (first PCE coefficient is excluded).</p>
</div>
</div>
<div class="section" id="momentestimation-class-descriptions">
<h3>MomentEstimation Class Descriptions<a class="headerlink" href="#momentestimation-class-descriptions" title="Permalink to this headline">¶</a></h3>
<div class="line-block">
<div class="line"><br /></div>
</div>
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><ol class="upperalpha simple" start="13">
<li><p>Grigoriu, “Reduced order models for random functions. Application to stochastic problems”, Applied Mathematical Modelling, Volume 33, Issue 1, Pages 161-175, 2009.</p></li>
</ol>
</dd>
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><ol class="upperalpha simple" start="14">
<li><p>Lüthen, S. Marelli, B. Sudret, “Sparse Polynomial Chaos Expansions: Solvers, Basis Adaptivity and Meta-selection“, Available at arXiv:2009.04800v1 [stat.CO], 2020.</p></li>
</ol>
</dd>
</dl>
<div class="toctree-wrapper compound">
</div>
</div>
</div>
</div>


          </div>
          
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="main navigation">
        <div class="sphinxsidebarwrapper">
<h1 class="logo"><a href="index.html">aaaaa</a></h1>








<h3>Navigation</h3>
<ul>
<li class="toctree-l1"><a class="reference internal" href="installation_doc.html">Introduction</a></li>
</ul>

<div class="relations">
<h3>Related Topics</h3>
<ul>
  <li><a href="index.html">Documentation overview</a><ul>
  </ul></li>
</ul>
</div>
<div id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" />
      <input type="submit" value="Go" />
    </form>
    </div>
</div>
<script>$('#searchbox').show(0);</script>








        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="footer">
      &copy;2020, aaaa.
      
      |
      Powered by <a href="http://sphinx-doc.org/">Sphinx 3.4.3</a>
      &amp; <a href="https://github.com/bitprophet/alabaster">Alabaster 0.7.12</a>
      
      |
      <a href="_sources/surrogates_doc.rst.txt"
          rel="nofollow">Page source</a>
    </div>

    

    
  </body>
</html>